{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62947b19-ba30-4f5d-b0c1-edd0f446feed",
   "metadata": {},
   "source": [
    "# Intro and Notes\n",
    "If you plan to run this on your own machine, please update the following code block with the appropriate directories, filenames, and options. The rest of the notebook can run seamlessly.\n",
    "\n",
    "**Please note** that updating all the states took ~19 hours for an i9-7980XE. If you only need to update specific states or already have the state CSVs, be sure to indicate that in the cell below.\n",
    "\n",
    "County shapefiles for 2020 were retrieved from this link: \n",
    "https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_county_500k.zip\n",
    "\n",
    "County FEMA data was retrieved from this link:\n",
    "https://hazards.fema.gov/nri/Content/StaticDocuments/DataDownload//NRI_Table_Counties/NRI_Table_Counties.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd53e25-0fbb-4fdf-8f6c-49c7afd8c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Directory + Shapefile Name for US County boundaries\n",
    "shapefile_path = 'counties/cb_2020_us_county_500k.shp'\n",
    "\n",
    "# Set Directory for Microsoft Building Footprint GeoJSON files\n",
    "geojson_dir = 'msft_geojson/'\n",
    "\n",
    "# Define export path for the by-state CSVs with MSFT/OSM areas, as well as the unmapped statistic by county\n",
    "export_path = 'export_csv/'\n",
    "\n",
    "# Set path for consolidated CSV with all counties and all areas\n",
    "consolidated_area_csv = 'all_counties_with_areas.csv'\n",
    "\n",
    "# Set path for FEMA data\n",
    "fema_csv = 'fema_data/NRI_Table_Counties.csv'\n",
    "\n",
    "# Set path for the final CSV with the priority statistic\n",
    "pri_stat_export_csv = 'priority_statistic_df.csv'\n",
    "\n",
    "# Define Notebook Run mode and whether you are updating all or some states\n",
    "method = 'some' #some or all\n",
    "states = ['Rhode Island', 'District of Columbia', 'Nevada'] #include the states of interest. You may use a blank list if you already have all CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b4f6d-7c22-4578-abb7-b4c05a339e03",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d738a65-3d44-4f2d-afec-b5ba8ad23bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import fiona, json, re\n",
    "\n",
    "from shapely.geometry import shape, Polygon, MultiPolygon, Point\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab79f0cf-bffc-4a05-8cc7-83ec5fc9de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import county boundaries\n",
    "master_df = gpd.read_file(shapefile_path)\n",
    "master_df = master_df[['GEOID', 'NAMELSAD', 'STATE_NAME', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa9f7f4-f189-4d67-aadb-9fc0be9b9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 50 States + DC into the official regions/divisions\n",
    "\n",
    "#Region 1: Northeast\n",
    "div1 = ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island','Vermont']\n",
    "div2 = ['New Jersey', 'New York', 'Pennsylvania']\n",
    "\n",
    "#Region 2: Midwest\n",
    "div3 = ['Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin']\n",
    "div4 = ['Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota']\n",
    "\n",
    "#Region 3: South\n",
    "div5 = ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'District of Columbia', 'West Virginia']\n",
    "div6 = ['Alabama', 'Kentucky', 'Mississippi', 'Tennessee']\n",
    "div7 = ['Arkansas', 'Louisiana', 'Oklahoma', 'Texas']\n",
    "\n",
    "#Region 4: West\n",
    "div8 = ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming']\n",
    "div9 = ['Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']\n",
    "\n",
    "divs = [div1, div2, div3, div4, div5, div6, div7, div8, div9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af282ed-3c26-459b-a4f5-05ae4fa0d1f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function Definitions\n",
    "## OSM Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d53e71cc-abb7-4606-8525-13eecf769202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_polygons(row, state_df):\n",
    "    # Use the master shapefile to query OSM by county shape\n",
    "    mask = state_df[state_df['GEOID']==row['GEOID']].iloc[0]['geometry']\n",
    "    \n",
    "    # Return the polygons within the county that aren't points\n",
    "    try: \n",
    "        bld = ox.geometries.geometries_from_polygon(mask,  tags = {'building': True})\n",
    "        bld = bld[bld['geometry'].apply(lambda x: not isinstance(x, Point))]\n",
    "        return bld[['geometry']]\n",
    "    except ox._errors.EmptyOverpassResponse:\n",
    "        print(str(row['NAMELSAD'])+' was empty')\n",
    "        return gpd.GeoDataFrame()\n",
    "    except:\n",
    "        print(\"Something else went wrong for \"+ str(row['NAMELSAD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560423d8-64dc-4a4d-9304-1f95c631331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_osm_areas(state):\n",
    "    # Create a temporary dataframe with the county polygons for the current state\n",
    "    state_shp_df = master_df[master_df['STATE_NAME'] == state]\n",
    "    \n",
    "    # For each county in the state, query the overpass API\n",
    "    df = gpd.GeoDataFrame()\n",
    "    for index, row in state_shp_df.iterrows():\n",
    "        df = pd.concat((df, get_county_polygons(row,state_shp_df)))\n",
    "    \n",
    "    # Calculate the area of each of the polygons\n",
    "    df = df.set_crs(crs=4326, allow_override=True).to_crs(epsg=2163)\n",
    "    df = df.where(df['geometry'].is_valid)\n",
    "    df['element_area'] = df['geometry'].area\n",
    "    df = df.to_crs(crs=4326)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dfc79-7311-4da0-b197-0a75b32b6c35",
   "metadata": {},
   "source": [
    "## MSFT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2f4351-de0a-40f1-a4c2-40e4a8b0d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msft_areas(state):\n",
    "    # Read the geoJSON file in for the state\n",
    "    geojson_file = geojson_dir+state.replace(' ','')+'.geojson'\n",
    "    df = gpd.read_file(geojson_file)\n",
    "    df = pd.concat({state:df}, names=['STATE_NAME'])\n",
    "    \n",
    "    # Calculate the areas of each polygon\n",
    "    df = df.set_crs(crs=4326, allow_override=True).to_crs(epsg=2163)\n",
    "    df = df.where(df['geometry'].is_valid)\n",
    "    df['element_area'] = df['geometry'].area\n",
    "    df = df.to_crs(crs=4326)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a649cb-ce3d-4a81-aba6-7796a9855358",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3e990a-cf04-4fee-82d2-4fb590bd68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_area_sum(row, df):\n",
    "    try:\n",
    "        # Create a mask using the county polygon made from the shapefile dataframe\n",
    "        mask = row['geometry']\n",
    "        gdf = df.clip(mask)\n",
    "        \n",
    "        # Sum up the areas found in that county\n",
    "        return gdf['element_area'].sum()\n",
    "    except:\n",
    "        print('Issue for '+row['NAMELSAD']+ ' in ' + row['STATE_NAME'])\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409324b5-a897-40db-b85e-f538bd91f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_areas(state):\n",
    "    \n",
    "    # Create a state dataframe with rows representing counties\n",
    "    query_df = master_df[master_df['STATE_NAME']==state].copy()\n",
    "    osm_df = get_osm_areas(state)\n",
    "    msft_df = get_msft_areas(state)\n",
    "    \n",
    "    # Add a column with the Microsoft building area calculation\n",
    "    query_df['msft_area'] = query_df.apply(lambda row: get_county_area_sum(row, msft_df), axis=1)\n",
    "    \n",
    "    # Add a column with the OSM building area calculation\n",
    "    query_df['osm_area'] = query_df.apply(lambda row: get_county_area_sum(row, osm_df), axis=1)\n",
    "    \n",
    "    # Calculate \"unmapped\" percentage based on area\n",
    "    query_df['percent_unmapped'] = 100*((query_df['msft_area']-query_df['osm_area'])/(query_df['msft_area']))\n",
    "    \n",
    "    # Save as export csv\n",
    "    query_df.to_csv(export_path+state.replace(' ','')+'.csv')\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c996668-0948-4acd-9c5a-cc3b0f0325ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Functions per Division\n",
    "The end state for this section is to have CSVs per state with each row representing a county. Rows will have the Microsoft building area, OSM building area, and unmapped statistic calculated from those two areas. This code block will also generate a consolidated CSV with all counties in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65148dbd-ea27-4372-885b-0f3a9e4ebe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_states(method='some', states = ['']):\n",
    "    area_df = pd.DataFrame()\n",
    "\n",
    "    if method == 'all':\n",
    "        for div in divs:\n",
    "            for state in div:\n",
    "                area_df = pd.concat([area_df, calc_areas(state)])\n",
    "                \n",
    "    else:\n",
    "        for state in states:\n",
    "            calc_areas(state)\n",
    "\n",
    "        def add_csv(main_df, file):\n",
    "            df = pd.read_csv(file, converters = {'GEOID':str})\n",
    "            main_df = pd.concat([main_df, df])\n",
    "            return main_df\n",
    "\n",
    "        file_list = sorted(glob(export_path + '*.csv'))\n",
    "        for file in file_list:\n",
    "            area_df = add_csv(area_df, file)\n",
    "\n",
    "    area_df.to_csv(consolidated_area_csv)\n",
    "    return area_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d724f4-358d-4eed-9d0b-54d13d125f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n",
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n",
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n",
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n",
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n",
      "C:\\Users\\catim\\anaconda3\\lib\\site-packages\\geopandas\\tools\\clip.py:67: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  clipped.loc[\n"
     ]
    }
   ],
   "source": [
    "area_df = update_states(method, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8466d0e-0598-41a2-9e42-3ee01bde88b5",
   "metadata": {},
   "source": [
    "# FEMA Data Import and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86243925-1c1b-461e-b685-d6c536c3343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fema_df = pd.read_csv(fema_csv, converters = {\"STCOFIPS\":str}).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0480a17b-b6ed-4483-b3ec-2f331477190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The lists below identifies all the risks to merge in the dataframe.\n",
    "risks = [\"STCOFIPS\", \"RISK_SCORE\", \"AVLN_RISKS\", \"CFLD_RISKS\", \"CWAV_RISKS\", \"DRGT_RISKS\", \"ERQK_RISKS\", \"HAIL_RISKS\", \n",
    "         \"HWAV_RISKS\", \"HRCN_RISKS\", \"ISTM_RISKS\", \"LNDS_RISKS\", \"LTNG_RISKS\", \"RFLD_RISKS\", \"SWND_RISKS\",\"TRND_RISKS\",\n",
    "         \"TSUN_RISKS\", \"VLCN_RISKS\", \"WFIR_RISKS\", \"WNTW_RISKS\"]\n",
    "names = [\"avalache_score\", \"coastal_flooding_score\", \"cold_wave_score\", \"drought_score\", \"earthquake_score\", \"hail_score\", \"heat_wave_score\", \"hurricane_score\", \n",
    "         \"ice_storm_score\", \"landslide_score\", \"lightning_score\", \"riverine_flooding_score\", \"strong_wind_score\", \"tornado_score\", \"tsunami_score\", \"volcano_score\", \n",
    "         \"wildfire_score\", \"winter_weather_score\"]\n",
    "\n",
    "fema_df = fema_df[risks] ## Select only relevant risk columns\n",
    "df = area_df.merge(fema_df, left_on=\"GEOID\", right_on=\"STCOFIPS\", how=\"left\").drop(\"STCOFIPS\", axis=1)\n",
    "\n",
    "undermapped_ratios = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "pri_stat_df = pd.DataFrame()\n",
    "temp_df = df.copy()\n",
    "for ratio in undermapped_ratios:\n",
    "    for i in range(len(names)):\n",
    "        temp_df[names[i]] = temp_df[risks[i+2]]*ratio + temp_df.percent_unmapped*(1-ratio)\n",
    "    temp_df[\"ratio\"] = ratio*100\n",
    "    pri_stat_df = pd.concat([pri_stat_df,temp_df])\n",
    "    \n",
    "pri_stat_df.to_csv(pri_stat_export_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
